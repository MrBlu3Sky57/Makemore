{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "train_end = round(len(words) * 0.8)\n",
    "dev_end = round((len(words) - train_end)/2) + train_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set for bigrams\n",
    "bxs, bys = [], []\n",
    "for w in words[:train_end]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        bxs.append(ix1)\n",
    "        bys.append(ix2)\n",
    "\n",
    "bxs = torch.tensor(bxs)\n",
    "bys = torch.tensor(bys)\n",
    "bnum = bxs.nelement()\n",
    "\n",
    "# Initalize network\n",
    "bg = torch.Generator().manual_seed(2147483647)\n",
    "bW = torch.randn((27, 27), generator=bg, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set for trigrams\n",
    "tx1 = []\n",
    "tx2 = []\n",
    "tys = []\n",
    "tnum = 0\n",
    "\n",
    "for w in words[:train_end]:\n",
    "    chs = ['.', '.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        id1 = stoi[ch1]\n",
    "        id2 = stoi[ch2]\n",
    "        id3 = stoi[ch3]\n",
    "\n",
    "        tx1.append(id1)\n",
    "        tx2.append(id2 + 27)\n",
    "        tys.append(id3)\n",
    "\n",
    "tx1 = torch.tensor(tx1)\n",
    "tx2 = torch.tensor(tx2)\n",
    "tys = torch.tensor(tys)\n",
    "tnum = tys.nelement()\n",
    "\n",
    "# Initalize Network\n",
    "tg = torch.Generator().manual_seed(2147483647)\n",
    "tW = torch.randn((54, 27), generator=tg, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIGRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.425746202468872\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "for k in range(1000):\n",
    "    # Forward Pass\n",
    "    xenc = F.one_hot(bxs, num_classes=27).float() # Input to the network: one-hot encoding\n",
    "    logits = xenc @ bW # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdim=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(bnum), bys].log().mean()\n",
    "\n",
    "    # Backward pass\n",
    "    bW.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    bW.data += -50 * bW.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIGRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.342146873474121\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(tx1, num_classes=54).float() + F.one_hot(tx2, num_classes=54).float()\n",
    "\n",
    "for k in range(1000):\n",
    "    logits = xenc @ tW\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(tnum), tys].log().mean()\n",
    "\n",
    "    tW.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    tW.data += -50 * tW.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dev set for bigrams\n",
    "bxs, bys = [], []\n",
    "for w in words[train_end:dev_end]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        bxs.append(ix1)\n",
    "        bys.append(ix2)\n",
    "\n",
    "bxs = torch.tensor(bxs)\n",
    "bys = torch.tensor(bys)\n",
    "bnum = bxs.nelement()\n",
    "\n",
    "# Initalize network\n",
    "bg = torch.Generator().manual_seed(2147483647)\n",
    "bW = torch.randn((27, 27), generator=bg, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dev set for trigrams\n",
    "tx1 = []\n",
    "tx2 = []\n",
    "tys = []\n",
    "tnum = 0\n",
    "\n",
    "for w in words[train_end:dev_end]:\n",
    "    chs = ['.', '.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        id1 = stoi[ch1]\n",
    "        id2 = stoi[ch2]\n",
    "        id3 = stoi[ch3]\n",
    "\n",
    "        tx1.append(id1)\n",
    "        tx2.append(id2 + 27)\n",
    "        tys.append(id3)\n",
    "\n",
    "tx1 = torch.tensor(tx1)\n",
    "tx2 = torch.tensor(tx2)\n",
    "tys = torch.tensor(tys)\n",
    "tnum = tys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.533405303955078\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "for k in range(100):\n",
    "    # Forward Pass\n",
    "    xenc = F.one_hot(bxs, num_classes=27).float() # Input to the network: one-hot encoding\n",
    "    logits = xenc @ bW # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdim=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(bnum), bys].log().mean() + 0.01 * (bW**2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    bW.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    bW.data += -30 * bW.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.386350631713867\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(tx1, num_classes=54).float() + F.one_hot(tx2, num_classes=54).float()\n",
    "\n",
    "for k in range(100):\n",
    "    logits = xenc @ tW\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(tnum), tys].log().mean() + 0.01 * (tW**2).mean()\n",
    "\n",
    "    tW.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    tW.data += -30 * tW.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set for bigrams\n",
    "bxs, bys = [], []\n",
    "for w in words[dev_end:]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        bxs.append(ix1)\n",
    "        bys.append(ix2)\n",
    "\n",
    "bxs = torch.tensor(bxs)\n",
    "bys = torch.tensor(bys)\n",
    "bnum = bxs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set for trigrams\n",
    "tx1 = []\n",
    "tx2 = []\n",
    "tys = []\n",
    "tnum = 0\n",
    "\n",
    "for w in words[dev_end:]:\n",
    "    chs = ['.', '.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        id1 = stoi[ch1]\n",
    "        id2 = stoi[ch2]\n",
    "        id3 = stoi[ch3]\n",
    "\n",
    "        tx1.append(id1)\n",
    "        tx2.append(id2 + 27)\n",
    "        tys.append(id3)\n",
    "\n",
    "tx1 = torch.tensor(tx1)\n",
    "tx2 = torch.tensor(tx2)\n",
    "tys = torch.tensor(tys)\n",
    "tnum = tys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5418505668640137\n"
     ]
    }
   ],
   "source": [
    "# Bigram Forward Pass\n",
    "xenc = F.one_hot(bxs, num_classes=27).float() # Input to the network: one-hot encoding\n",
    "logits = xenc @ bW # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(bnum), bys].log().mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41545033454895\n"
     ]
    }
   ],
   "source": [
    "# Trigram Forward Pass\n",
    "xenc = F.one_hot(tx1, num_classes=54).float() + F.one_hot(tx2, num_classes=54).float()    \n",
    "logits = xenc @ tW\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(tnum), tys].log().mean()\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jigla\n",
      "sadrqr\n",
      "brixzydavesolen\n",
      "pshabasha\n",
      "n\n",
      "r\n",
      "mas\n",
      "tharze\n",
      "brylon\n",
      "kym\n"
     ]
    }
   ],
   "source": [
    "# Bigram Output Testing\n",
    "\n",
    "for x in range(10):\n",
    "    cur = torch.zeros(27)\n",
    "    cur[0] = 1\n",
    "\n",
    "    chars = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        logits = cur @ bW # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum() # probabilities for next character\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=bg).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        chars.append(itos[ix])\n",
    "        cur = torch.zeros(27)\n",
    "        cur[ix] = 1\n",
    "\n",
    "\n",
    "    print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aellen\n",
      "fedyrocre\n",
      "was\n",
      "kon\n",
      "num\n",
      "blu\n",
      "jeaglon\n",
      "asiani\n",
      "lamaron\n",
      "wan\n"
     ]
    }
   ],
   "source": [
    "# Trigram Output Testing\n",
    "\n",
    "for k in range(10):\n",
    "    input = torch.zeros(54)\n",
    "    f = 0\n",
    "    s = 0\n",
    "\n",
    "    out = []\n",
    "    while True:\n",
    "        logits = input @ tW\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=tg).item()\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "        out.append(itos[idx])\n",
    "\n",
    "        f = s\n",
    "        s = idx\n",
    "\n",
    "        input = torch.zeros(54)\n",
    "        input[f] = 1\n",
    "        input[s + 27] = 1\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
